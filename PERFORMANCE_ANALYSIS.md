# Performance Analysis Report

**Date:** 2025-12-18
**Codebase:** gemini-web-wrapper
**Total Lines of Code:** ~3,354 lines

## Executive Summary

This analysis identifies performance anti-patterns, inefficiencies, and optimization opportunities across the gemini-web-wrapper codebase. The codebase is generally well-structured with proper async/await patterns and thread pool usage, but several critical performance issues were found.

---

## Critical Issues

### 1. **Fake Streaming Implementation** (HIGH PRIORITY)

**Location:** `server.py:669-681`, `server.py:783-832`

**Issue:**
The streaming endpoints don't actually stream responses. They generate the complete response and send it as a single chunk, defeating the purpose of streaming.

```python
# server.py:679
async def generate_stream() -> AsyncGenerator[str, None]:
    response = await run_generate(msgs, model)
    yield response.text  # Sends entire response at once!
```

```python
# server.py:803
chunk_data = {
    "delta": {
        "role": "assistant",
        "content": text,  # Entire text in one chunk
    }
}
```

**Impact:**
- Users don't get real-time streaming experience
- Increased latency perception
- Memory inefficiency for large responses
- Misleading API contract

**Recommendation:**
- Implement true token-by-token streaming if Genkit/Gemini supports it
- If not supported, split response into chunks (e.g., by sentences or fixed size)
- Document limitation if streaming is unavailable

---

### 2. **Inefficient JSON Parsing with O(nÂ²) Fallback**

**Location:** `openai_transforms.py:76-121`

**Issue:**
The `_extract_json_with_tool_calls()` function has an optimized path but falls back to trying every brace position in the text.

```python
# openai_transforms.py:107-119
# Fallback: try all braces if backward search fails (rare edge case)
start = 0
while start < len(text):
    brace_idx = text.find("{", start)
    if brace_idx < 0:
        break
    try:
        obj, end_idx = decoder.raw_decode(text, brace_idx)
        if "tool_calls" in obj:
            return text[brace_idx : brace_idx + end_idx]
        start = brace_idx + 1
    except json.JSONDecodeError:
        start = brace_idx + 1
```

**Impact:**
- O(n*m) time complexity in worst case (n = text length, m = JSON objects)
- For responses with many braces or large text, this becomes expensive
- Each failed parse attempt creates overhead

**Recommendation:**
- Limit fallback attempts (e.g., max 10 positions)
- Add early exit if text is too large (> 10KB)
- Consider regex pre-scan to identify likely JSON boundaries

---

### 3. **Database Query Anti-pattern**

**Location:** `cookie_manager.py:365-402`

**Issue:**
`load_profile()` makes two separate database queries instead of using a JOIN.

```python
# cookie_manager.py:376-390
# Query 1: Get profile metadata
cursor = await db.execute(
    "SELECT * FROM profiles WHERE name = ?",
    (profile_name,),
)
profile_row = await cursor.fetchone()

# Query 2: Get cookies
cursor = await db.execute(
    "SELECT * FROM cookies WHERE profile_name = ?",
    (profile_name,),
)
cookie_rows = await cursor.fetchall()
```

**Impact:**
- Two round-trips to database
- Potential for inconsistency between queries
- Slower profile loading, especially under concurrent load

**Recommendation:**
```python
cursor = await db.execute("""
    SELECT p.*, c.name as cookie_name, c.value, c.domain,
           c.path, c.expires, c.secure, c.http_only
    FROM profiles p
    LEFT JOIN cookies c ON p.name = c.profile_name
    WHERE p.name = ?
""", (profile_name,))
```

---

### 4. **Global Lock Contention in Cookie Manager**

**Location:** `cookie_manager.py:194`

**Issue:**
All database operations use the same lock, creating a bottleneck under concurrent load.

```python
# cookie_manager.py:194
async with self._lock, aiosqlite.connect(self.db_path) as db:
```

**Impact:**
- All DB operations are serialized
- Profile listing blocks profile loading
- Read operations block other reads unnecessarily
- Scalability bottleneck for multi-user scenarios

**Recommendation:**
- Use read/write lock pattern (asyncio.Lock for writes, no lock for reads)
- SQLite's WAL mode allows concurrent reads
- Enable WAL: `PRAGMA journal_mode=WAL`
- Separate locks for different operations (profiles vs cookies)

---

## Moderate Issues

### 5. **Unnecessary Profile Query in Cookie Extraction**

**Location:** `cookie_manager.py:452-483`

**Issue:**
`get_gemini_cookies()` loads the entire profile object to extract only 2-3 cookie values.

```python
# cookie_manager.py:461
profile = await self.load_profile(profile_name)
# Loads all cookies, metadata, timestamps...
# But only uses 2 cookies: __Secure-1PSID, __Secure-1PSIDTS
```

**Impact:**
- Loads unnecessary data from database
- More memory allocation
- Slower response time

**Recommendation:**
```python
async def get_gemini_cookies(self, profile_name: str) -> dict[str, str] | None:
    """Get only required Gemini cookies efficiently."""
    async with self._db_connection() as db:
        cursor = await db.execute("""
            SELECT name, value, expires
            FROM cookies
            WHERE profile_name = ?
              AND name IN (?, ?)
        """, (profile_name, *self.REQUIRED_COOKIES))
        # ... process results
```

---

### 6. **Inefficient Message Collapsing**

**Location:** `openai_transforms.py:241-283`

**Issue:**
`collapse_messages()` builds multiple intermediate lists and performs multiple string operations.

```python
# openai_transforms.py:243-278
system_prompts: list[str] = []
dialogue_lines: list[str] = []

for message in request.messages:
    # ... multiple appends and string formatting
    dialogue_lines.append(f"{prefix}: {rendered}")

prompt_sections: list[str] = []
if system_prompts:
    prompt_sections.append("\n".join(system_prompts))
prompt_sections.append("\n".join(dialogue_lines))

base_prompt = "\n\n".join(section for section in prompt_sections if section)
```

**Impact:**
- Multiple list allocations
- Multiple string join operations
- O(n) iterations over messages with string copying

**Recommendation:**
- Use a single list with pre-calculated capacity
- Use StringIO for large message sets
- Avoid intermediate joins, build final string once

---

### 7. **Cache Key Inefficiency**

**Location:** `server.py:389-403`

**Issue:**
Cache key construction could create many duplicate entries when `session_id` is `None`.

```python
# server.py:390-391
effective_user_id = user_id or "default_user"
cache_key = (effective_user_id, session_id)
```

**Impact:**
- Cache keys like `("default_user", None)` could accumulate
- Multiple None session_ids not deduplicated
- Cache memory grows unnecessarily

**Recommendation:**
```python
# Normalize None to a sentinel value
effective_session_id = session_id or "__no_session__"
cache_key = (effective_user_id, effective_session_id)
```

---

### 8. **Redundant Attribution Calls**

**Location:** `server.py:371-403`

**Issue:**
Even with caching, attribution setup makes two separate thread pool calls.

```python
# server.py:395-401
await run_in_thread(
    memori.attribution,
    entity_id=effective_user_id,
    process_id="gemini-chatbot",
)
if session_id:
    await run_in_thread(memori.set_session, session_id)
```

**Impact:**
- Two thread pool operations per attribution
- Double context switching overhead
- Unnecessary for cached attributions

**Recommendation:**
- Combine operations if Memori API allows
- Cache session setup separately if frequently called with same session

---

### 9. **Text Rendering Inefficiency**

**Location:** `openai_transforms.py:223-238`

**Issue:**
`render_message_content()` uses list comprehension that filters inline.

```python
# openai_transforms.py:230-237
return "\n".join(
    [
        part.text
        for part in content
        if isinstance(part, ChatCompletionMessageContent)
        and part.type == "text"
        and part.text is not None
    ]
)
```

**Impact:**
- Creates intermediate list before joining
- Multiple type checks per part
- Memory allocation for filtered list

**Recommendation:**
```python
# Use generator expression (no intermediate list)
return "\n".join(
    part.text
    for part in content
    if isinstance(part, ChatCompletionMessageContent)
       and part.type == "text"
       and part.text is not None
)
```

---

## Minor Issues

### 10. **Potential Memory Issue in List Operations**

**Location:** `gemini_client.py:272-276`

**Issue:**
Returns empty list on exception, but doesn't limit result size.

```python
# gemini_client.py:272-275
conversations = await asyncio.to_thread(
    self.client.list_conversations,
)
return conversations if conversations else []
```

**Impact:**
- Large conversation lists could consume significant memory
- No pagination or limiting
- Could cause memory pressure under high load

**Recommendation:**
- Add pagination support
- Limit default result size (e.g., 100 conversations)
- Document memory considerations

---

### 11. **Cookie Extraction Timeout**

**Location:** `cookie_manager.py:277-290`

**Issue:**
Default 10-second timeout might be too long for a synchronous operation.

```python
# cookie_manager.py:279-285
return await asyncio.wait_for(
    asyncio.to_thread(
        self._extract_cookies_from_browser,
        browser,
        domain,
    ),
    timeout=timeout,
)
```

**Impact:**
- Blocks event loop for up to 10 seconds
- Other requests wait during cookie extraction
- User experience degrades

**Recommendation:**
- Reduce default timeout to 3-5 seconds
- Add retry with smaller timeout
- Document that cookie extraction should be done during setup, not per-request

---

### 12. **Inefficient Tool Format Building**

**Location:** `openai_transforms.py:47-60`

**Issue:**
`format_tools_for_prompt()` builds intermediate dictionaries for JSON serialization.

```python
# openai_transforms.py:49-59
tools_data = [
    {
        "type": tool.type,
        "function": {
            "name": tool.function.name,
            "description": tool.function.description,
            "parameters": tool.function.parameters,
        },
    }
    for tool in tools
]
return json.dumps(tools_data, indent=2)
```

**Impact:**
- Creates intermediate dictionary structures
- Memory allocation for nested dicts
- Could use direct JSON building for large tool sets

**Recommendation:**
- For tools list > 10, consider streaming JSON serialization
- Use `json.dumps([tool.dict() for tool in tools])` if Pydantic models
- Cache formatted tools if reused across requests

---

## Good Practices Observed

### âœ… Proper Async/Await Usage
- Blocking operations correctly wrapped in `asyncio.to_thread()`
- Consistent use of async context managers
- Thread pool executor for CPU-bound tasks

### âœ… Database Optimizations
- `WITHOUT ROWID` tables (lines 158, 174 in cookie_manager.py)
- Index on frequently joined columns (line 179)
- `executemany()` for batch inserts (line 350)
- WAL mode mentioned but implementation not verified

### âœ… Caching Strategy
- TTLCache for attribution (server.py:118)
- 1-hour TTL prevents unbounded growth
- Appropriate cache size (10,000 entries)

### âœ… Error Handling
- Comprehensive exception handling
- Proper error propagation
- Timeout protection on long operations

### âœ… Type Hints
- Consistent type annotations throughout
- Protocol classes for interface definition
- Proper use of Optional and Union types

---

## Summary of Recommendations by Priority

### High Priority
1. âš ï¸ **Fix fake streaming** - Implement real streaming or document limitation
2. âš ï¸ **Optimize JSON parsing fallback** - Add limits and early exits
3. âš ï¸ **Fix database query pattern** - Use JOINs instead of separate queries
4. âš ï¸ **Reduce lock contention** - Implement read/write lock pattern

### Medium Priority
5. ðŸ”§ Optimize `get_gemini_cookies()` - Query only needed cookies
6. ðŸ”§ Improve message collapsing efficiency - Reduce intermediate allocations
7. ðŸ”§ Fix cache key normalization - Handle None session_id properly
8. ðŸ”§ Combine attribution calls - Reduce thread pool overhead

### Low Priority
9. ðŸ“ Add pagination to conversation listing
10. ðŸ“ Reduce cookie extraction timeout
11. ðŸ“ Optimize tool format building for large tool sets
12. ðŸ“ Add memory limits documentation

---

## Performance Metrics Suggestions

To measure improvements, consider tracking:
- Request latency (p50, p95, p99)
- Database query time
- Lock wait time
- Memory usage per request
- Concurrent request throughput
- Cache hit rate for attribution

---

## Testing Recommendations

1. **Load Testing**
   - Simulate 100+ concurrent requests
   - Measure response times under load
   - Profile lock contention

2. **Memory Profiling**
   - Track memory usage with large conversation histories
   - Monitor for memory leaks in long-running processes
   - Test profile operations with 1000+ cookies

3. **Benchmark Key Operations**
   - Message collapsing with 50+ messages
   - JSON parsing with large responses (>50KB)
   - Database operations under concurrent load
   - Cookie extraction from different browsers

---

## Conclusion

The codebase demonstrates good async patterns and some optimization awareness (caching, indexing, batch operations). However, critical issues like fake streaming, O(nÂ²) fallbacks, and lock contention need immediate attention. Addressing high-priority issues will significantly improve performance under load.

**Estimated Impact of Fixes:**
- Streaming fix: 30-50% improvement in perceived latency
- JSON parsing: 20-40% faster for tool call responses
- DB query optimization: 15-25% faster profile operations
- Lock optimization: 2-3x throughput improvement under concurrent load
